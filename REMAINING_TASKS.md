# ğŸ”§ JokboDude ë‚¨ì€ ì‘ì—… - ê°œë°œì ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ](#í˜„ì¬-ì‹œìŠ¤í…œ-ìƒíƒœ)
2. [ì¦‰ì‹œ í•´ê²° í•„ìš” ì‚¬í•­](#ì¦‰ì‹œ-í•´ê²°-í•„ìš”-ì‚¬í•­)
3. [ê¸°ëŠ¥ ê°œì„  ì‚¬í•­](#ê¸°ëŠ¥-ê°œì„ -ì‚¬í•­)
4. [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)
5. [í™•ì¥ ê¸°ëŠ¥](#í™•ì¥-ê¸°ëŠ¥)
6. [ì½”ë“œ í’ˆì§ˆ ê°œì„ ](#ì½”ë“œ-í’ˆì§ˆ-ê°œì„ )
7. [ë°°í¬ ë° ìš´ì˜](#ë°°í¬-ë°-ìš´ì˜)

## í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ

### ì•„í‚¤í…ì²˜ ê°œìš”
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚â”€â”€â”€â”€â–¶â”‚ Web Service â”‚â”€â”€â”€â”€â–¶â”‚    Redis    â”‚
â”‚   (HTML)    â”‚     â”‚  (FastAPI)  â”‚     â”‚  (Storage)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚                    â–²
                            â–¼                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
                    â”‚   Celery    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚   Worker    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### í•µì‹¬ íŒŒì¼ êµ¬ì¡°
```
jokbodude/
â”œâ”€â”€ web_server.py          # FastAPI ì›¹ ì„œë²„
â”œâ”€â”€ tasks.py               # Celery ì›Œì»¤ íƒœìŠ¤í¬
â”œâ”€â”€ storage_manager.py     # Redis ìŠ¤í† ë¦¬ì§€ ê´€ë¦¬
â”œâ”€â”€ pdf_processor/         # PDF ì²˜ë¦¬ ì—”ì§„
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ processor.py   # ë©”ì¸ ì²˜ë¦¬ ë¡œì§
â”‚   â”œâ”€â”€ analyzers/         # ë¶„ì„ ëª¨ë“œë³„ êµ¬í˜„
â”‚   â”œâ”€â”€ api/              # Gemini API ê´€ë¦¬
â”‚   â””â”€â”€ parsers/          # ì‘ë‹µ íŒŒì‹±
â”œâ”€â”€ pdf_creator.py        # PDF ìƒì„±
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ index.html        # ì›¹ ì¸í„°í˜ì´ìŠ¤
â””â”€â”€ render.yaml           # Render ë°°í¬ ì„¤ì •
```

## ì¦‰ì‹œ í•´ê²° í•„ìš” ì‚¬í•­

### 1. Lesson-Centric ëª¨ë“œ Redis ì§€ì› ë¯¸ì™„ì„± âš ï¸
**íŒŒì¼:** `tasks.py`
**ë¼ì¸:** 111-150 (ëŒ€ëµ)
**ë¬¸ì œ:** `run_lesson_analysis` í•¨ìˆ˜ê°€ ì•„ì§ ì´ì „ ë°©ì‹ ì‚¬ìš© ì¤‘

**ìˆ˜ì • í•„ìš” ì½”ë“œ:**
```python
@celery_app.task(name="tasks.run_lesson_analysis")
def run_lesson_analysis(job_id: str, model_type: str = None):
    """Run lesson-centric analysis"""
    storage_manager = StorageManager()
    
    try:
        # Get job metadata from Redis (jokbo_analysisì™€ ë™ì¼í•œ íŒ¨í„´)
        metadata = storage_manager.get_job_metadata(job_id)
        if not metadata:
            raise Exception(f"Job metadata not found for {job_id}")
        
        # ì´í•˜ jokbo_analysisì™€ ìœ ì‚¬í•˜ê²Œ êµ¬í˜„
        # lesson_pathsë¥¼ ìˆœíšŒí•˜ë©° ì²˜ë¦¬
        # ê° lessonì— ëŒ€í•´ create_lesson_centric_pdf í˜¸ì¶œ
```

### 2. Web Server Lesson-Centric ì—”ë“œí¬ì¸íŠ¸ ìˆ˜ì • í•„ìš” âš ï¸
**íŒŒì¼:** `web_server.py`
**ë¼ì¸:** 111-140 (ëŒ€ëµ)
**ë¬¸ì œ:** `analyze_lesson_centric` í•¨ìˆ˜ê°€ ì´ì „ ë°©ì‹ ì‚¬ìš©

**ìˆ˜ì • í•„ìš”:**
- jokbo-centricê³¼ ë™ì¼í•œ íŒ¨í„´ìœ¼ë¡œ ìˆ˜ì •
- Redisë¥¼ í†µí•œ íŒŒì¼ ì €ì¥ êµ¬í˜„

### 3. ê²°ê³¼ íŒŒì¼ ëª©ë¡ API ìˆ˜ì • âš ï¸
**íŒŒì¼:** `web_server.py`
**í•¨ìˆ˜:** `list_result_files`, `get_specific_result_file`
**ë¬¸ì œ:** ì•„ì§ ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ ì°¸ì¡°

**ìˆ˜ì • ë°©í–¥:**
```python
@app.get("/results/{job_id}")
def list_result_files(job_id: str):
    # Redisì—ì„œ ê²°ê³¼ í‚¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    result_keys = storage_manager.redis_client.keys(f"result:{job_id}:*")
    # íŒŒì¼ëª… ì¶”ì¶œ ë° ë°˜í™˜
```

## ê¸°ëŠ¥ ê°œì„  ì‚¬í•­

### 1. ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™” ğŸ”´
**ìš°ì„ ìˆœìœ„: ë†’ìŒ**

**í˜„ì¬ ë¬¸ì œì :**
- Redis ì—°ê²° ì‹¤íŒ¨ ì‹œ ì ì ˆí•œ ì—ëŸ¬ ë©”ì‹œì§€ ì—†ìŒ
- íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë¡¤ë°± ì—†ìŒ
- Gemini API ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ë¡œì§ ë¶€ì¡±

**ê°œì„  ë°©ì•ˆ:**
```python
# storage_manager.pyì— ì¶”ê°€
class StorageManager:
    def __init__(self, redis_url: str = None):
        try:
            self.redis_client = redis.from_url(redis_url)
            self.redis_client.ping()  # ì—°ê²° í…ŒìŠ¤íŠ¸
        except redis.ConnectionError:
            # Fallback to local storage or raise appropriate error
            logger.error("Redis connection failed, using local storage")
            self.use_local_only = True

    def store_file_with_retry(self, file_path, job_id, file_type, max_retries=3):
        for attempt in range(max_retries):
            try:
                return self.store_file(file_path, job_id, file_type)
            except Exception as e:
                if attempt == max_retries - 1:
                    raise
                time.sleep(2 ** attempt)  # Exponential backoff
```

### 2. í”„ë¡œê·¸ë ˆìŠ¤ íŠ¸ë˜í‚¹ ê°œì„  ğŸŸ¡
**ìš°ì„ ìˆœìœ„: ì¤‘ê°„**

**êµ¬í˜„ í•„ìš”:**
```python
# tasks.pyì— ì¶”ê°€
def update_progress(job_id: str, progress: int, message: str):
    storage_manager.redis_client.hset(
        f"progress:{job_id}",
        mapping={
            "progress": progress,
            "message": message,
            "timestamp": datetime.now().isoformat()
        }
    )

# ì²˜ë¦¬ ì¤‘ í˜¸ì¶œ
update_progress(job_id, 25, "íŒŒì¼ ë¶„ì„ ì‹œì‘")
update_progress(job_id, 50, "AI ì²˜ë¦¬ ì¤‘")
update_progress(job_id, 75, "PDF ìƒì„± ì¤‘")
update_progress(job_id, 100, "ì™„ë£Œ")
```

**Frontend ì—…ë°ì´íŠ¸:**
```javascript
// ì‹¤ì‹œê°„ ì§„í–‰ë¥  í™•ì¸
async function checkProgress(jobId) {
    const res = await fetch(`/progress/${jobId}`);
    const data = await res.json();
    updateProgressBar(data.progress, data.message);
}
```

### 3. íŒŒì¼ ê²€ì¦ ê°•í™” ğŸŸ¡
**ìš°ì„ ìˆœìœ„: ì¤‘ê°„**

**ì¶”ê°€ í•„ìš” ê²€ì¦:**
- PDF íŒŒì¼ ìœ íš¨ì„± (ì‹¤ì œ PDFì¸ì§€)
- í˜ì´ì§€ ìˆ˜ ì œí•œ (ì˜ˆ: 500í˜ì´ì§€)
- íŒŒì¼ëª… sanitization
- ì¤‘ë³µ íŒŒì¼ ì²´í¬

```python
# web_server.pyì— ì¶”ê°€
import PyPDF2

def validate_pdf(file_content: bytes) -> bool:
    try:
        pdf = PyPDF2.PdfReader(io.BytesIO(file_content))
        if len(pdf.pages) > 500:
            raise ValueError("PDF exceeds 500 pages")
        return True
    except:
        return False
```

## ì„±ëŠ¥ ìµœì í™”

### 1. Redis ë©”ëª¨ë¦¬ ìµœì í™” ğŸŸ 
**ë¬¸ì œ:** í° PDF íŒŒì¼ë“¤ì´ Redis ë©”ëª¨ë¦¬ ì´ˆê³¼ ê°€ëŠ¥

**í•´ê²° ë°©ì•ˆ 1: ì••ì¶• ì‚¬ìš©**
```python
import zlib

def store_file_compressed(self, file_path: Path, job_id: str, file_type: str):
    with open(file_path, 'rb') as f:
        content = f.read()
    
    # ì••ì¶•
    compressed = zlib.compress(content, level=6)
    compression_ratio = len(compressed) / len(content)
    
    if compression_ratio < 0.9:  # 10% ì´ìƒ ì••ì¶•ë˜ë©´ ì‚¬ìš©
        self.redis_client.hset(
            f"file:{job_id}:{file_type}",
            mapping={
                "data": compressed,
                "compressed": "true",
                "original_size": len(content)
            }
        )
```

**í•´ê²° ë°©ì•ˆ 2: ì²­í¬ ë¶„í• **
```python
def store_large_file(self, file_path: Path, job_id: str, chunk_size=5*1024*1024):
    # 5MB ì²­í¬ë¡œ ë¶„í• 
    chunks = []
    with open(file_path, 'rb') as f:
        while chunk := f.read(chunk_size):
            chunk_id = str(uuid.uuid4())
            self.redis_client.setex(f"chunk:{chunk_id}", 3600, chunk)
            chunks.append(chunk_id)
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    self.redis_client.hset(
        f"file:{job_id}:{file_path.name}",
        mapping={"chunks": json.dumps(chunks)}
    )
```

### 2. ë³‘ë ¬ ì²˜ë¦¬ ê°œì„  ğŸŸ 
**í˜„ì¬:** ìˆœì°¨ ì²˜ë¦¬
**ê°œì„ :** ë™ì‹œ ì²˜ë¦¬

```python
# tasks.py ê°œì„ 
from concurrent.futures import ThreadPoolExecutor

def process_multiple_files(file_paths, processor):
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = []
        for path in file_paths:
            future = executor.submit(process_single_file, path, processor)
            futures.append(future)
        
        results = [f.result() for f in futures]
    return results
```

### 3. ìºì‹± ì „ëµ ğŸŸ¢
**ìš°ì„ ìˆœìœ„: ë‚®ìŒ**

```python
# ê²°ê³¼ ìºì‹±
def get_cached_result(file_hash: str):
    cached = redis_client.get(f"cache:result:{file_hash}")
    if cached:
        logger.info(f"Cache hit for {file_hash}")
        return cached
    return None

# íŒŒì¼ í•´ì‹œ ìƒì„±
def get_file_hash(content: bytes) -> str:
    return hashlib.sha256(content).hexdigest()
```

## í™•ì¥ ê¸°ëŠ¥

### 1. S3/R2 ìŠ¤í† ë¦¬ì§€ ì§€ì› ğŸŒŸ
**ì¥ì :** ëŒ€ìš©ëŸ‰ íŒŒì¼, ì˜êµ¬ ì €ì¥, ë¹„ìš© íš¨ìœ¨

```python
# storage_backends.py
import boto3

class S3StorageBackend:
    def __init__(self, bucket_name, aws_access_key, aws_secret_key):
        self.s3 = boto3.client('s3', 
            aws_access_key_id=aws_access_key,
            aws_secret_access_key=aws_secret_key
        )
        self.bucket = bucket_name
    
    def store_file(self, file_path: Path, key: str):
        self.s3.upload_file(str(file_path), self.bucket, key)
        return f"s3://{self.bucket}/{key}"
    
    def get_file(self, key: str) -> bytes:
        response = self.s3.get_object(Bucket=self.bucket, Key=key)
        return response['Body'].read()
```

### 2. ì‚¬ìš©ì ì¸ì¦ ì‹œìŠ¤í…œ ğŸŒŸ
```python
# auth.py
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
import jwt

oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

async def get_current_user(token: str = Depends(oauth2_scheme)):
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=["HS256"])
        user_id = payload.get("sub")
        if user_id is None:
            raise HTTPException(status_code=401)
        return user_id
    except jwt.JWTError:
        raise HTTPException(status_code=401)

# ì—”ë“œí¬ì¸íŠ¸ì— ì ìš©
@app.post("/analyze/jokbo-centric")
async def analyze_jokbo_centric(
    current_user: str = Depends(get_current_user),
    ...
):
```

### 3. ì›¹í›… ì•Œë¦¼ ğŸŒŸ
```python
# notifications.py
import httpx

async def send_webhook(url: str, job_id: str, status: str):
    async with httpx.AsyncClient() as client:
        await client.post(url, json={
            "job_id": job_id,
            "status": status,
            "timestamp": datetime.now().isoformat()
        })

# tasks.pyì—ì„œ ì‚¬ìš©
async def notify_completion(job_id: str):
    webhook_url = storage_manager.get_job_metadata(job_id).get("webhook_url")
    if webhook_url:
        await send_webhook(webhook_url, job_id, "completed")
```

### 4. ë°°ì¹˜ ì²˜ë¦¬ API ğŸŒŸ
```python
@app.post("/analyze/batch")
async def analyze_batch(
    batch_file: UploadFile = File(...),
    model: str = Query("flash")
):
    # CSV ë˜ëŠ” JSONìœ¼ë¡œ ì—¬ëŸ¬ ì‘ì—… ì •ì˜
    batch_data = parse_batch_file(batch_file)
    
    tasks = []
    for job in batch_data:
        task = celery_app.send_task(
            "tasks.run_batch_analysis",
            args=[job]
        )
        tasks.append(task.id)
    
    return {"batch_id": str(uuid.uuid4()), "tasks": tasks}
```

## ì½”ë“œ í’ˆì§ˆ ê°œì„ 

### 1. íƒ€ì… íŒíŒ… ì™„ì„± ğŸ”µ
```python
from typing import Dict, List, Optional, Union, Any
from pathlib import Path

def process_file(
    file_path: Union[str, Path],
    options: Optional[Dict[str, Any]] = None
) -> Dict[str, Union[str, int, List[str]]]:
    ...
```

### 2. ë¡œê¹… ì‹œìŠ¤í…œ ê°œì„  ğŸ”µ
```python
# logging_config.py
import logging
from logging.handlers import RotatingFileHandler

def setup_logging():
    logger = logging.getLogger("jokbodude")
    logger.setLevel(logging.INFO)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬
    file_handler = RotatingFileHandler(
        "jokbodude.log",
        maxBytes=10485760,  # 10MB
        backupCount=5
    )
    
    # í¬ë§·í„°
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    file_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    return logger
```

### 3. í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„± ğŸ”µ
```python
# tests/test_storage_manager.py
import pytest
from storage_manager import StorageManager

@pytest.fixture
def storage_manager():
    return StorageManager("redis://localhost:6379/1")  # Test DB

def test_store_and_retrieve_file(storage_manager, tmp_path):
    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
    test_file = tmp_path / "test.pdf"
    test_file.write_bytes(b"test content")
    
    # ì €ì¥
    key = storage_manager.store_file(test_file, "test_job", "test")
    
    # ê²€ìƒ‰
    content = storage_manager.get_file(key)
    assert content == b"test content"

def test_job_metadata(storage_manager):
    metadata = {"test": "data"}
    storage_manager.store_job_metadata("test_job", metadata)
    
    retrieved = storage_manager.get_job_metadata("test_job")
    assert retrieved == metadata
```

### 4. ë¬¸ì„œí™” ê°œì„  ğŸ”µ
```python
def analyze_jokbo_centric(
    lesson_paths: List[str],
    jokbo_path: str
) -> Dict[str, Any]:
    """
    ì¡±ë³´ ì¤‘ì‹¬ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        lesson_paths: ê°•ì˜ìë£Œ PDF íŒŒì¼ ê²½ë¡œ ëª©ë¡
        jokbo_path: ë¶„ì„í•  ì¡±ë³´ PDF íŒŒì¼ ê²½ë¡œ
    
    Returns:
        ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬:
        {
            "connections": [...],  # ì—°ê²° ì •ë³´
            "error": str,          # ì—ëŸ¬ ë°œìƒ ì‹œ
            "metadata": {...}      # ë©”íƒ€ë°ì´í„°
        }
    
    Raises:
        FileNotFoundError: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ
        ValueError: ì˜ëª»ëœ PDF íŒŒì¼ì¼ ë•Œ
        APIError: Gemini API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
    
    Example:
        >>> result = analyze_jokbo_centric(
        ...     ["lecture1.pdf", "lecture2.pdf"],
        ...     "exam.pdf"
        ... )
        >>> print(result["connections"])
    """
```

## ë°°í¬ ë° ìš´ì˜

### 1. í™˜ê²½ë³„ ì„¤ì • ë¶„ë¦¬ ğŸŸ£
```python
# config/settings.py
from pydantic import BaseSettings

class Settings(BaseSettings):
    # ê³µí†µ ì„¤ì •
    app_name: str = "JokboDude"
    
    # í™˜ê²½ë³„ ì„¤ì •
    redis_url: str
    gemini_api_key: str
    storage_backend: str = "redis"  # redis, s3, local
    
    # ê°œë°œ/í”„ë¡œë•ì…˜ êµ¬ë¶„
    debug: bool = False
    
    class Config:
        env_file = ".env"
        env_prefix = "JOKBO_"

# í™˜ê²½ë³„ íŒŒì¼
# .env.development
# .env.production
# .env.test
```

### 2. í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ ğŸŸ£
```python
@app.get("/health")
async def health_check():
    checks = {
        "web": "healthy",
        "redis": "unknown",
        "worker": "unknown"
    }
    
    # Redis ì²´í¬
    try:
        storage_manager.redis_client.ping()
        checks["redis"] = "healthy"
    except:
        checks["redis"] = "unhealthy"
    
    # Worker ì²´í¬
    try:
        result = celery_app.control.inspect().active()
        checks["worker"] = "healthy" if result else "unhealthy"
    except:
        checks["worker"] = "unhealthy"
    
    status_code = 200 if all(v == "healthy" for v in checks.values()) else 503
    return JSONResponse(content=checks, status_code=status_code)
```

### 3. ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ğŸŸ£
```python
# metrics.py
from prometheus_client import Counter, Histogram, generate_latest

# ë©”íŠ¸ë¦­ ì •ì˜
job_counter = Counter('jokbodude_jobs_total', 'Total number of jobs', ['status'])
processing_time = Histogram('jokbodude_processing_seconds', 'Processing time')

# ì‚¬ìš©
job_counter.labels(status='started').inc()

@app.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type="text/plain")
```

### 4. ìë™ ì •ë¦¬ ì‘ì—… ğŸŸ£
```python
# cleanup_task.py
@celery_app.task
def cleanup_old_data():
    """1ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ë˜ëŠ” ì •ë¦¬ ì‘ì—…"""
    # ì˜¤ë˜ëœ Redis í‚¤ ì‚­ì œ
    for key in redis_client.scan_iter("file:*"):
        ttl = redis_client.ttl(key)
        if ttl == -1:  # TTL ì—†ìŒ
            redis_client.expire(key, 3600)
    
    # ì˜¤ë˜ëœ ë¡œì»¬ íŒŒì¼ ì‚­ì œ
    storage_path = Path("/tmp/storage")
    for job_dir in storage_path.iterdir():
        if job_dir.is_dir():
            age = time.time() - job_dir.stat().st_mtime
            if age > 7200:  # 2ì‹œê°„ ì´ìƒ
                shutil.rmtree(job_dir)

# Celery beat ìŠ¤ì¼€ì¤„
from celery.schedules import crontab

celery_app.conf.beat_schedule = {
    'cleanup-old-data': {
        'task': 'cleanup_task.cleanup_old_data',
        'schedule': crontab(minute=0),  # ë§¤ ì‹œê°„
    },
}
```

## ë³´ì•ˆ ê°œì„  ì‚¬í•­

### 1. ì…ë ¥ ê²€ì¦ ê°•í™” ğŸ”’
```python
from pydantic import BaseModel, validator

class AnalysisRequest(BaseModel):
    model: str
    webhook_url: Optional[str] = None
    
    @validator('model')
    def validate_model(cls, v):
        if v != 'flash':
            raise ValueError('Invalid model (only flash supported)')
        return v
    
    @validator('webhook_url')
    def validate_webhook(cls, v):
        if v and not v.startswith('https://'):
            raise ValueError('Webhook must use HTTPS')
        return v
```

### 2. Rate Limiting ğŸ”’
```python
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(
    key_func=get_remote_address,
    default_limits=["100 per hour"]
)

@app.post("/analyze/jokbo-centric")
@limiter.limit("10 per minute")
async def analyze_jokbo_centric(...):
```

### 3. íŒŒì¼ ì—…ë¡œë“œ ë³´ì•ˆ ğŸ”’
```python
ALLOWED_EXTENSIONS = {'.pdf'}
BLOCKED_FILENAMES = {'../', '..\\', '/etc/', 'C:\\'}

def secure_filename(filename: str) -> str:
    # ìœ„í—˜í•œ ë¬¸ì ì œê±°
    filename = re.sub(r'[^\w\s.-]', '', filename)
    
    # ê²½ë¡œ íƒìƒ‰ ë°©ì§€
    for blocked in BLOCKED_FILENAMES:
        if blocked in filename:
            raise ValueError("Invalid filename")
    
    # í™•ì¥ì í™•ì¸
    if not any(filename.endswith(ext) for ext in ALLOWED_EXTENSIONS):
        raise ValueError("Only PDF files allowed")
    
    return filename
```

## ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### 1. Sentry í†µí•© ğŸ“Š
```python
import sentry_sdk
from sentry_sdk.integrations.fastapi import FastApiIntegration
from sentry_sdk.integrations.celery import CeleryIntegration

sentry_sdk.init(
    dsn="your-sentry-dsn",
    integrations=[
        FastApiIntegration(transaction_style="endpoint"),
        CeleryIntegration()
    ],
    traces_sample_rate=0.1,
)
```

### 2. ì‚¬ìš©ëŸ‰ ì¶”ì  ğŸ“Š
```python
# usage_tracking.py
def track_usage(user_id: str, job_id: str, file_count: int, total_size: int):
    redis_client.hincrby(f"usage:{user_id}:daily", "jobs", 1)
    redis_client.hincrby(f"usage:{user_id}:daily", "files", file_count)
    redis_client.hincrby(f"usage:{user_id}:daily", "bytes", total_size)
    
    # ì¼ì¼ ë¦¬ì…‹
    redis_client.expire(f"usage:{user_id}:daily", 86400)

def check_usage_limit(user_id: str) -> bool:
    usage = redis_client.hgetall(f"usage:{user_id}:daily")
    
    # ê¸°ë³¸ ì œí•œ
    MAX_DAILY_JOBS = 100
    MAX_DAILY_BYTES = 500 * 1024 * 1024  # 500MB
    
    if int(usage.get(b'jobs', 0)) >= MAX_DAILY_JOBS:
        return False
    if int(usage.get(b'bytes', 0)) >= MAX_DAILY_BYTES:
        return False
    
    return True
```

## ê¸´ê¸‰ ìˆ˜ì • í•„ìš” ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì¦‰ì‹œ ìˆ˜ì • (ë°°í¬ ì „ í•„ìˆ˜)
- [ ] `tasks.py`ì˜ `run_lesson_analysis` Redis ì§€ì›
- [ ] `web_server.py`ì˜ `analyze_lesson_centric` Redis ì§€ì›
- [ ] ê²°ê³¼ ë‹¤ìš´ë¡œë“œ API Redis ëŒ€ì‘

### 1ì£¼ì¼ ë‚´ ìˆ˜ì • ê¶Œì¥
- [ ] ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”
- [ ] ì§„í–‰ë¥  íŠ¸ë˜í‚¹ êµ¬í˜„
- [ ] íŒŒì¼ ê²€ì¦ ê°•í™”
- [ ] í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸

### ì¥ê¸° ê°œì„  ì‚¬í•­
- [ ] S3/R2 ìŠ¤í† ë¦¬ì§€ ì§€ì›
- [ ] ì‚¬ìš©ì ì¸ì¦ ì‹œìŠ¤í…œ
- [ ] í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±
- [ ] ëª¨ë‹ˆí„°ë§ í†µí•©

## ê°œë°œ í™˜ê²½ ì„¤ì •

### ë¡œì»¬ ê°œë°œ í™˜ê²½
```bash
# Redis ì‹¤í–‰
docker run -d -p 6379:6379 redis:alpine

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export REDIS_URL=redis://localhost:6379/0
export GEMINI_API_KEY=your_key_here
export RENDER_STORAGE_PATH=/tmp/storage

# ì„œë¹„ìŠ¤ ì‹¤í–‰
# Terminal 1: Web
uvicorn web_server:app --reload

# Terminal 2: Worker
celery -A tasks worker --loglevel=info

# Terminal 3: Redis Monitor
redis-cli monitor
```

### ë””ë²„ê¹… íŒ
1. Redis ë°ì´í„° í™•ì¸: `redis-cli keys "*"`
2. Celery íƒœìŠ¤í¬ ìƒíƒœ: `celery -A tasks inspect active`
3. FastAPI ë¬¸ì„œ: `http://localhost:8000/docs`

---

**ì‘ì„±ì¼:** 2025-08-11
**ë²„ì „:** 1.0.0
**ì‘ì„±ì:** Claude (Anthropic)
**ëŒ€ìƒ:** ì°¨ì„¸ëŒ€ AI ì½”ë”© ì—ì´ì „íŠ¸
